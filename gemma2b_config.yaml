dataset:
  name: HuggingFaceH4/deita-10k-v0-sft #HuggingFaceH4/no_robots
  train_split: train_sft
  val_split: test_sft

model_name_or_path: google/gemma-2b

quantization:
  load_in_4bit: True
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: False

model_args:
  torch_dtype: bfloat16
  # trust_remote_code: True
  attn_implementation: sdpa #flash_attention_2
  use_cache: False

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  bias: none
  task_type: CAUSAL_LM
  target_modules:
    [
      "q_proj",
      "o_proj",
      "k_proj",
      "v_proj",
      "gate_proj",
      "up_proj",
      "down_proj",
    ]

training_args:
  output_dir: ./output/
  bf16: True
  fp16: False
  do_eval: True
  max_grad_norm: 1.0
  weight_decay: 0.001
  evaluation_strategy: steps
  gradient_accumulation_steps: 1 #4
  gradient_checkpointing: True
  gradient_checkpointing_kwargs: { "use_reentrant": False }
  learning_rate: 0.00002
  logging_steps: 200
  logging_strategy: steps
  lr_scheduler_type: cosine
  num_train_epochs: 1
  max_steps: -1
  overwrite_output_dir: True
  per_device_eval_batch_size: 4
  per_device_train_batch_size: 4
  save_strategy: epoch
  save_steps: 100
  save_total_limit: 1
  warmup_ratio: 0.1
  optim: paged_adamw_32bit
  report_to: none
  seed: 42
