dataset:
  name: Universal-NER/Pile-NER-type
  train_split: train
  val_split: test

model_name_or_path: google/gemma-2b

chat_template: "{% if messages[0]['role'] == 'user' or messages[0]['role'] == 'system' %}{{ bos_token }}{% endif %}{% for message in messages %}{{ '<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n' }}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% elif messages[-1]['role'] == 'assistant' %}{{ eos_token }}{% endif %}"
tokenizer_special_tokens:
  - <|im_start|>
  - <|im_end|>

quantization:
  load_in_4bit: True
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: False

model_args:
  torch_dtype: bfloat16
  # trust_remote_code: True
  attn_implementation: flash_attention_2
  use_cache: False

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  bias: none
  task_type: CAUSAL_LM
  target_modules:
    [
      "q_proj",
      "o_proj",
      "k_proj",
      "v_proj",
      "gate_proj",
      "up_proj",
      "down_proj",
    ]

training_args:
  output_dir: ./output/
  bf16: True
  fp16: False
  do_eval: True
  max_grad_norm: 1.0
  weight_decay: 0.001
  evaluation_strategy: steps
  gradient_accumulation_steps: 1 #4
  gradient_checkpointing: True
  gradient_checkpointing_kwargs: { "use_reentrant": False }
  learning_rate: 0.00002
  logging_steps: 200
  logging_strategy: steps
  lr_scheduler_type: cosine
  num_train_epochs: 2
  max_steps: -1
  overwrite_output_dir: True
  per_device_eval_batch_size: 4
  per_device_train_batch_size: 2
  save_strategy: epoch
  save_steps: 100
  save_total_limit: 1
  warmup_ratio: 0.1
  optim: paged_adamw_32bit
  report_to: none
  seed: 42
